# LLM_reproduction
Reproduction of diverse LLMs models


# Notes:

* trained with waaaay less computational power than any of the LLM (even the tutorial) did. Only 4GPU of ram. (E.g: had to run the MultiHeadAttentionModel training phase on 32 batch processing at the same time, 64 like the tutorial is too much for my gpu RAM)